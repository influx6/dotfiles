{
  "litellm_proxy/claude-sonnet-4": {
  "model_group": "claude-sonnet-4",
  "providers": [
    "vertex_ai"
  ],
  "max_input_tokens": 200000,
  "max_output_tokens": 64000,
  "input_cost_per_token": 0.000003,
  "output_cost_per_token": 0.000015,
  "mode": "chat",
  "tpm": null,
  "rpm": null,
  "supports_parallel_function_calling": false,
  "supports_vision": true,
  "supports_web_search": false,
  "supports_reasoning": true,
  "supports_function_calling": true,
  "supported_openai_params": [
    "stream",
    "stop",
    "temperature",
    "top_p",
    "max_tokens",
    "max_completion_tokens",
    "tools",
    "tool_choice",
    "extra_headers",
    "parallel_tool_calls",
    "response_format",
    "user",
    "reasoning_effort"
  ],
  "configurable_clientside_auth_params": null
  },
  "litellm_proxy/claude-opus-4": {
    "model_group": "claude-opus-4",
  "providers": [
    "vertex_ai"
  ],
  "max_input_tokens": 200000,
  "max_output_tokens": 32000,
  "input_cost_per_token": 0.000015,
  "output_cost_per_token": 0.000075,
  "mode": "chat",
  "tpm": null,
  "rpm": null,
  "supports_parallel_function_calling": false,
  "supports_vision": true,
  "supports_web_search": false,
  "supports_reasoning": true,
  "supports_function_calling": true,
  "supported_openai_params": [
    "stream",
    "stop",
    "temperature",
    "top_p",
    "max_tokens",
    "max_completion_tokens",
    "tools",
    "tool_choice",
    "extra_headers",
    "parallel_tool_calls",
    "response_format",
    "user",
    "reasoning_effort"
  ],
  "configurable_clientside_auth_params": null
  },
  "litellm_proxy/gemini-2.5-flash": {
    "model_name": "gemini-2.5-flash",
    "litellm_params": {
      "vertex_project": "cph-ops-sre",
      "vertex_location": "us-west1",
      "use_in_pass_through": false,
      "use_litellm_proxy": false,
      "merge_reasoning_content_in_choices": false,
      "model": "vertex_ai/gemini-2.5-flash"
    },
    "model_info": {
      "id": "5fe6dc8b6a529f8e83c2fc74569b93356c8a11e3e84b874d648450aa5be5b2b1",
      "db_model": false,
      "key": "gemini-2.5-flash",
      "max_tokens": 65535,
      "max_input_tokens": 1048576,
      "max_output_tokens": 65535,
      "input_cost_per_token": 3e-7,
      "cache_creation_input_token_cost": null,
      "cache_read_input_token_cost": null,
      "input_cost_per_character": null,
      "input_cost_per_token_above_128k_tokens": null,
      "input_cost_per_token_above_200k_tokens": null,
      "input_cost_per_query": null,
      "input_cost_per_second": null,
      "input_cost_per_audio_token": 0.000001,
      "input_cost_per_token_batches": null,
      "output_cost_per_token_batches": null,
      "output_cost_per_token": 0.0000025,
      "output_cost_per_audio_token": null,
      "output_cost_per_character": null,
      "output_cost_per_reasoning_token": 0.0000025,
      "output_cost_per_token_above_128k_tokens": null,
      "output_cost_per_character_above_128k_tokens": null,
      "output_cost_per_token_above_200k_tokens": null,
      "output_cost_per_second": null,
      "output_cost_per_image": null,
      "output_vector_size": null,
      "litellm_provider": "vertex_ai-language-models",
      "mode": "chat",
      "supports_system_messages": true,
      "supports_response_schema": true,
      "supports_vision": true,
      "supports_function_calling": true,
      "supports_tool_choice": true,
      "supports_assistant_prefill": null,
      "supports_prompt_caching": null,
      "supports_audio_input": null,
      "supports_audio_output": false,
      "supports_pdf_input": null,
      "supports_embedding_image_input": null,
      "supports_native_streaming": null,
      "supports_web_search": true,
      "supports_url_context": true,
      "supports_reasoning": true,
      "supports_computer_use": null,
      "search_context_cost_per_query": null,
      "tpm": null,
      "rpm": null,
      "supported_openai_params": [
        "temperature",
        "top_p",
        "max_tokens",
        "max_completion_tokens",
        "stream",
        "tools",
        "functions",
        "tool_choice",
        "response_format",
        "n",
        "stop",
        "frequency_penalty",
        "presence_penalty",
        "extra_headers",
        "seed",
        "logprobs",
        "top_logprobs",
        "modalities",
        "parallel_tool_calls",
        "web_search_options",
        "reasoning_effort",
        "thinking"
      ]
    },
    "provider": "vertex_ai",
    "input_cost": "0.30",
    "output_cost": "2.50",
    "litellm_model_name": "vertex_ai/gemini-2.5-flash",
    "max_tokens": 65535,
    "max_input_tokens": 1048576,
    "cleanedLitellmParams": {
      "vertex_project": "cph-ops-sre",
      "vertex_location": "us-west1",
      "use_in_pass_through": false,
      "use_litellm_proxy": false,
      "merge_reasoning_content_in_choices": false
    }
  }
}
